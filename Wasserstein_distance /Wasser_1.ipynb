{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB4GsFgLC0363oxY7JHz+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamid-Mofidi/Synchronization-of-periodic-orbits/blob/main/Wasserstein_distance%20/Wasser_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Wasserstein distance**:\n",
        "\n",
        "The Wasserstein distance is a way of measuring how different two probability distributions are. It is also known as the earth mover's distance, because it can be seen as the minimum amount of work needed to transform one distribution into another, where work is defined as the amount of mass moved times the distance it is moved. For example, if we have two piles of sand with different shapes, the Wasserstein distance between them is the minimum amount of sand we need to shovel from one place to another to make the piles have the same shape.\n",
        "\n",
        "The **Frechet** distance is a special case of the Wasserstein distance when the distributions are Gaussian, which means they have a bell-shaped curve. In this case, the Wasserstein distance can be computed in a simple way using the means and covariances of the distributions. The mean is the average value of the distribution, and the covariance is a measure of how much the values vary around the mean. The Frechet distance between two Gaussian distributions is given by a formula that involves the squared difference of the means and the trace and determinant of the covariances.\n",
        "\n",
        "Other special cases of the Wasserstein distance are:\n",
        "\n",
        "•  The **total variation distance**, which is the Wasserstein distance when the cost function is 0 if two points are equal and 1 otherwise. This means that we only care about whether two distributions assign the same probability to each point, not how far apart they are.\n",
        "\n",
        "•  The **Kullback-Leibler divergence**, which is the Wasserstein distance when the cost function is the logarithm of the ratio of two probabilities. This means that we care more about how different two distributions are when they assign low probabilities to some points, rather than high probabilities.\n",
        "\n",
        "•  The **Hellinger distance**, which is the Wasserstein distance when the cost function is the square root of 1 minus the square root of the product of two probabilities. This means that we care more about how different two distributions are when they assign high probabilities to some points, rather than low probabilities."
      ],
      "metadata": {
        "id": "20I9-fXOBvbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we have two probability distributions, P and Q, defined on the interval [0, 1].\n",
        "# We can represent these distributions as continuous functions, where the height of the function at any point represents the probability density at that point.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import quad\n",
        "\n",
        "# Define the probability distributions P and Q\n",
        "def P(x):\n",
        "    if 0 <= x <= 0.5:\n",
        "        return 2 * x\n",
        "    elif 0.5 < x <= 1:\n",
        "        return 2 - 2 * x\n",
        "\n",
        "def Q(x):\n",
        "    return 0.5\n",
        "\n",
        "# Calculate the CDFs of P and Q:\n",
        "# For 0 <= x <= 0.5:\n",
        "# CDF_P(x) = integral of P(t) dt from 0 to x\n",
        "#         = integral of (2t) dt from 0 to x\n",
        "#         = t^2 evaluated from 0 to x\n",
        "#         = x^2\n",
        "#########\n",
        "#For 0.5 < x <= 1:\n",
        "# CDF_P(x) = integral of P(t) dt from 0 to 0.5 + integral of P(t) dt from 0.5 to x\n",
        "#         = integral of (2t) dt from 0 to 0.5 + integral of (2 - 2t) dt from 0.5 to x\n",
        "#         = t^2 evaluated from 0 to 0.5 + (2t - t^2) evaluated from 0.5 to x\n",
        "#         = 0.25 + (2x - x^2) - (1 - 0.25)\n",
        "#         = 2x - x^2 - 0.5\n",
        "def CDF_P(x):\n",
        "    if 0 <= x <= 0.5:\n",
        "        return x**2\n",
        "    elif 0.5 < x <= 1:\n",
        "        return 2 * x - x**2 - 0.5\n",
        "\n",
        "\n",
        "\n",
        "def CDF_Q(x):\n",
        "    return 0.5 * x\n",
        "\n",
        "# Calculate the absolute difference between CDFs\n",
        "def abs_diff(x):\n",
        "    return abs(CDF_P(x) - CDF_Q(x))\n",
        "\n",
        "# Calculate the Wasserstein distance\n",
        "wasserstein_dist, _ = quad(abs_diff, 0, 1)\n",
        "\n",
        "print(\"Wasserstein distance:\", wasserstein_dist)\n",
        "\n",
        "# Running this code will output the Wasserstein distance between the distributions P and Q.\n",
        "# In this example, the Wasserstein distance is 1/24=0.04166666666."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybIAIVlrkRDL",
        "outputId": "a141b611-d351-4605-ce60-73aaa4a5d492"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wasserstein distance: 0.041666666666666664\n"
          ]
        }
      ]
    }
  ]
}